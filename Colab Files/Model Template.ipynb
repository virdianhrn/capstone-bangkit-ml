{"cells":[{"cell_type":"markdown","metadata":{"id":"dnVtPkZp6YLO"},"source":["# Preperation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xk8ANT-Psmt0"},"outputs":[],"source":["!pip install transformers\n","!pip install sentence-transformers\n","!gdown --folder \"1cszZtjGiWoS5kJEU3cF-VZaXDYMiV7KR\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKUsIn5VtGnI"},"outputs":[],"source":["from transformers import BertTokenizer, TFBertModel, TFRobertaModel, RobertaTokenizer\n","from sentence_transformers import SentenceTransformer\n","from nltk.cluster import KMeansClusterer\n","from nltk.cluster.util import cosine_distance\n","from sklearn.metrics import silhouette_score\n","\n","import tensorflow as tf\n","import keras\n","from keras import layers, metrics\n","import pandas as pd\n","import numpy as np\n","import math\n","import matplotlib as plt\n","from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"jjm0GWfLzWgo"},"source":["# Load Dataset"]},{"cell_type":"markdown","metadata":{"id":"wNdtUxlV36co"},"source":["## Function & Global Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXskiNdV4I2E"},"outputs":[],"source":["TRAIN_SIZE = 0.6\n","VAL_SIZE = 0.2\n","TEST_SIZE = 0.2\n","BATCH_SIZE = 1\n","\n","bert_base = 'indolem/indobert-base-uncased'\n","sentence_bert_base = 'firqaaa/indo-sentence-bert-base'\n","tokenizer = BertTokenizer.from_pretrained(bert_base)\n","sentence_transformer = SentenceTransformer(sentence_bert_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RepOtNk1Q7tj"},"outputs":[],"source":["def sentence_encode(text):\n","    return sentence_transformer(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-6laGHjRrEW"},"outputs":[],"source":["def get_max_len(texts):\n","  max_len = 0\n","  for text in texts:\n","    input_ids = tokenizer(text, add_special_tokens=True)['input_ids']\n","    max_len = max(max_len, len(input_ids))\n","\n","  # return 2 ** math.ceil(math.log2(max_len))\n","  # return max_len\n","  return min(2 ** math.ceil(math.log2(max_len)), 2048)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgPWq77c3r2u"},"outputs":[],"source":["def encode(text, max_len):\n","    encoded_dict = tokenizer(text, add_special_tokens = True, max_length = max_len,\n","                             padding='max_length', return_attention_mask = True,\n","                             truncation=True, return_tensors = 'tf')\n","\n","    input_ids = encoded_dict['input_ids']\n","    attention_masks = encoded_dict['attention_mask']\n","    return (input_ids, attention_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQucj0xShmWv"},"outputs":[],"source":["def split_input_labels(features, labels):\n","    return {'input_ids': features[0], 'attention_mask': features[1]}, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJLeYA8008lM"},"outputs":[],"source":["def split_dataset(dataset):\n","  train_val_dataset, test_dataset = tf.keras.utils.split_dataset(\n","    dataset, left_size=TRAIN_SIZE+VAL_SIZE, right_size=TEST_SIZE, shuffle=True, seed=42\n","  )\n","\n","  train_dataset, val_dataset = tf.keras.utils.split_dataset(\n","      train_val_dataset, left_size=TRAIN_SIZE, right_size=VAL_SIZE, shuffle=True, seed=42\n","  )\n","\n","  return (train_dataset.map(split_input_labels),\n","          val_dataset.map(split_input_labels),\n","          test_dataset.map(split_input_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qF7Muqpp4yrZ"},"outputs":[],"source":["def dataset_preperation(text, labels, max_len):\n","  features_tensor = encode(text.values.tolist(), max_len)\n","  labels_tensor = tf.constant(labels, dtype=tf.int32)\n","  dataset = tf.data.Dataset.from_tensor_slices((features_tensor, labels_tensor))\n","\n","  train, val, test = split_dataset(dataset)\n","\n","  return (train.batch(BATCH_SIZE),\n","          val.batch(BATCH_SIZE),\n","          test.batch(BATCH_SIZE))"]},{"cell_type":"markdown","metadata":{"id":"TMmooS0xRZ7e"},"source":["## Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBDIuLERRe2j"},"outputs":[],"source":["df_clustering = pd.read_csv('news.csv')\n","train_clustering = sentence_encode(df_clustering['text'])"]},{"cell_type":"markdown","metadata":{"id":"EC4JAkZrzdXp"},"source":["## Subjectivity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F60R3Be_zN5f"},"outputs":[],"source":["df_subjectivity = pd.read_csv('/content/Clean Dataset/subjectivity-MPQA-News.csv')\n","features = df_subjectivity['content']\n","max_len_subjectivity = get_max_len(features)\n","labels = df_subjectivity['is_subjective']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D0nf7235Q1N"},"outputs":[],"source":["train_subjectivity, val_subjectivity, test_subjectivity = dataset_preperation(features, labels, max_len_subjectivity)"]},{"cell_type":"markdown","metadata":{"id":"F41sSm-q5vH4"},"source":["## Bias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4y2b4AW5vH6"},"outputs":[],"source":["df_bias = pd.read_csv('Clean Dataset/bias-neutrality.csv')\n","features = df_bias['teks']\n","max_len_bias = get_max_len(features)\n","labels = df_bias['is_biased']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN8jod9d5vH6"},"outputs":[],"source":["train_bias, val_bias, test_bias = dataset_preperation(features, labels, max_len_bias)"]},{"cell_type":"markdown","metadata":{"id":"t-ZkWW4E56vS"},"source":["## Neutrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJsyIu9K56vT"},"outputs":[],"source":["df_neutrality = pd.read_csv('Clean Dataset/bias-neutrality.csv')\n","features = df_neutrality['teks']\n","max_len_neutrality = get_max_len(features)\n","labels = df_neutrality[['is_left', 'is_center', 'is_right']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W801ckz56vU"},"outputs":[],"source":["train_neutrality, val_neutrality, test_neutrality = dataset_preperation(features, labels, max_len_neutrality)"]},{"cell_type":"markdown","metadata":{"id":"CJbWNgvD6l--"},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{"id":"tCqdJlHIVH5X"},"source":["## Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAySbYFpEYHJ"},"outputs":[],"source":["def get_clusters(data, num_of_centroid=10):\n","  kclusterer = KMeansClusterer(num_of_centroid, distance=cosine_distance,\n","                               repeats=25, avoid_empty_clusters=True)\n","\n","  return kclusterer.cluster(data, assign_clusters=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGf_XK-WEua5"},"outputs":[],"source":["clusters = []\n","for i in range(1, 11):\n","  clusters.append(get_clusters(train_clustering, i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2r9HU9W8Fakv"},"outputs":[],"source":["silhouette = [silhouette_score(df_clustering['text'], cluster, metric='cosine') for cluster in clusters]\n","plt.plot(range(1, 11), silhouette)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkUfzsDbHi00"},"outputs":[],"source":["best_idx = np.array(silhouette).argmax()\n","df_clustering['cluster'] = pd.Series(clusters[best_idx], index=df_clustering.index)"]},{"cell_type":"markdown","metadata":{"id":"93JITKzHsGg9"},"source":["## Subjectivity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p01VbJResgAa"},"outputs":[],"source":["def create_subjectivity_model(max_len):\n","  input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n","  attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n","\n","  bert = TFBertModel.from_pretrained(bert_base, from_pt=True)(input_ids, attention_mask)\n","\n","  classifier = keras.Sequential([\n","        layers.Dense(256, input_shape=(768,), activation=\"relu\"),\n","        layers.Dense(32, activation=\"relu\"),\n","        layers.Dense(32, activation=\"relu\"),\n","        layers.Dense(1, activation='sigmoid')\n","  ])(bert.pooler_output)\n","\n","  # bert.trainable = False\n","  model = keras.Model(inputs=[input_ids, attention_mask],\n","                     outputs=classifier)\n","\n","  model.compile(loss='binary_crossentropy',\n","                optimizer='adam',\n","                metrics=['accuracy'])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5601,"status":"ok","timestamp":1701874254481,"user":{"displayName":"Virdian Harun Prayoga M010BSY0110","userId":"05344229873782265456"},"user_tz":-420},"id":"clrYpsxkzjHh","outputId":"ad655a02-259e-46a5-9add-6a7b69798001"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_7\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_ids (InputLayer)      [(None, 2048)]               0         []                            \n","                                                                                                  \n"," attention_mask (InputLayer  [(None, 2048)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," tf_bert_model_7 (TFBertMod  TFBaseModelOutputWithPooli   1105582   ['input_ids[0][0]',           \n"," el)                         ngAndCrossAttentions(last_   08         'attention_mask[0][0]']      \n","                             hidden_state=(None, 2048,                                            \n","                             768),                                                                \n","                              pooler_output=(None, 768)                                           \n","                             , past_key_values=None, hi                                           \n","                             dden_states=None, attentio                                           \n","                             ns=None, cross_attentions=                                           \n","                             None)                                                                \n","                                                                                                  \n"," sequential_7 (Sequential)   (None, 1)                    206177    ['tf_bert_model_7[0][1]']     \n","                                                                                                  \n","==================================================================================================\n","Total params: 110764385 (422.53 MB)\n","Trainable params: 110764385 (422.53 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["subjectivity_model = create_subjectivity_model(max_len_subjectivity)\n","subjectivity_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7c-1MN3CLfM3","outputId":"44151b5b-42d6-4355-ebcc-43bf31429f68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","404/404 [==============================] - 434s 939ms/step - loss: 0.7040 - accuracy: 0.5965 - val_loss: 0.6692 - val_accuracy: 0.6963\n","Epoch 2/30\n","404/404 [==============================] - 376s 931ms/step - loss: 0.6649 - accuracy: 0.6535 - val_loss: 0.6497 - val_accuracy: 0.6963\n","Epoch 3/30\n","404/404 [==============================] - 376s 930ms/step - loss: 0.6592 - accuracy: 0.6609 - val_loss: 0.6349 - val_accuracy: 0.6963\n","Epoch 4/30\n","404/404 [==============================] - 372s 921ms/step - loss: 0.6545 - accuracy: 0.6609 - val_loss: 0.6275 - val_accuracy: 0.6963\n","Epoch 5/30\n","404/404 [==============================] - 376s 930ms/step - loss: 0.6516 - accuracy: 0.6609 - val_loss: 0.6296 - val_accuracy: 0.6963\n","Epoch 6/30\n","404/404 [==============================] - 375s 930ms/step - loss: 0.6973 - accuracy: 0.6658 - val_loss: 0.6425 - val_accuracy: 0.6963\n","Epoch 7/30\n","404/404 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.6609"]}],"source":["subjectivity_model.fit(train_subjectivity, epochs=30, validation_data = val_subjectivity)"]},{"cell_type":"code","source":["subjectivity_model.save('subjectivity_model.keras')\n","files.download('subjectivity_model.keras')"],"metadata":{"id":"ExlSgbB0sKTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G2VNEE9gsKC8"},"source":["## Bias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qQmOP0A0eox"},"outputs":[],"source":["def create_bias_model(max_len):\n","  input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n","  attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n","\n","  bert = TFBertModel.from_pretrained(bert_base, from_pt=True)(input_ids, attention_mask)\n","\n","  classifier = keras.Sequential([\n","        layers.Dense(256, input_shape=(768,), activation=\"relu\"),\n","        layers.Dense(32, activation=\"relu\"),\n","        layers.Dense(32, activation=\"relu\"),\n","        layers.Dense(1, activation='sigmoid')\n","  ])(bert.pooler_output)\n","\n","  # bert.trainable = False\n","  model = keras.Model(inputs=[input_ids, attention_mask],\n","                     outputs=classifier)\n","\n","  model.compile(loss='binary_crossentropy',\n","                optimizer='adam',\n","                metrics=['accuracy'])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634,"referenced_widgets":["1c241ae5a70c4f72aadd4aa237af80ce","29978b5b20984dd5be8a773787857116","0a49668e7ac1405782b5e4db65d63fd1","a638c4b74d6b49f78d4463964312afd6","682ed2253eba4e4f9585a1b606de29ad","d7529e77a5004c1c83bf61077b1819fe","90691a9cd67d4f50bf51d33e8efe29e3","68707cfa67824fa48a83e2b0f6929d42","2df9c08c75c845e680297cd0e7f97703","cef09447899549f685b3791158e182da","9d53591ada9a4bccba121ab0f34000f1"]},"id":"V1hsM2410epE","executionInfo":{"status":"ok","timestamp":1701864309576,"user_tz":-420,"elapsed":14183,"user":{"displayName":"Virdian Harun Prayoga M010BSY0110","userId":"05344229873782265456"}},"outputId":"f09d2e09-0443-4b20-c9d4-74441052087e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c241ae5a70c4f72aadd4aa237af80ce"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_ids (InputLayer)      [(None, 128)]                0         []                            \n","                                                                                                  \n"," attention_mask (InputLayer  [(None, 128)]                0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1105582   ['input_ids[0][0]',           \n"," )                           ngAndCrossAttentions(last_   08         'attention_mask[0][0]']      \n","                             hidden_state=(None, 128, 7                                           \n","                             68),                                                                 \n","                              pooler_output=(None, 768)                                           \n","                             , past_key_values=None, hi                                           \n","                             dden_states=None, attentio                                           \n","                             ns=None, cross_attentions=                                           \n","                             None)                                                                \n","                                                                                                  \n"," sequential (Sequential)     (None, 1)                    206177    ['tf_bert_model[0][1]']       \n","                                                                                                  \n","==================================================================================================\n","Total params: 110764385 (422.53 MB)\n","Trainable params: 110764385 (422.53 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["bias_model = create_bias_model(max_len_bias)\n","bias_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkIQnGilZ8Vr","outputId":"130a8e35-2f9a-4b1d-d05e-8bcfe21c1133"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","95/95 [==============================] - 91s 505ms/step - loss: 0.7261 - accuracy: 0.5281 - val_loss: 0.7395 - val_accuracy: 0.4663\n","Epoch 2/30\n","95/95 [==============================] - 44s 465ms/step - loss: 0.7151 - accuracy: 0.5169 - val_loss: 0.7113 - val_accuracy: 0.4663\n","Epoch 3/30\n","95/95 [==============================] - 44s 464ms/step - loss: 0.6951 - accuracy: 0.5195 - val_loss: 0.6947 - val_accuracy: 0.5337\n","Epoch 4/30\n","95/95 [==============================] - 45s 469ms/step - loss: 0.6927 - accuracy: 0.5328 - val_loss: 0.6950 - val_accuracy: 0.5337\n","Epoch 5/30\n","95/95 [==============================] - 44s 467ms/step - loss: 0.6939 - accuracy: 0.5050 - val_loss: 0.6918 - val_accuracy: 0.5337\n","Epoch 6/30\n","95/95 [==============================] - 43s 457ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6910 - val_accuracy: 0.5337\n","Epoch 7/30\n","95/95 [==============================] - 43s 457ms/step - loss: 0.6920 - accuracy: 0.5314 - val_loss: 0.6920 - val_accuracy: 0.5337\n","Epoch 8/30\n","95/95 [==============================] - 44s 465ms/step - loss: 0.6923 - accuracy: 0.5314 - val_loss: 0.6923 - val_accuracy: 0.5337\n","Epoch 9/30\n","95/95 [==============================] - 43s 458ms/step - loss: 0.6921 - accuracy: 0.5314 - val_loss: 0.6935 - val_accuracy: 0.5337\n","Epoch 10/30\n","95/95 [==============================] - 44s 465ms/step - loss: 0.6921 - accuracy: 0.5301 - val_loss: 0.6949 - val_accuracy: 0.5337\n","Epoch 11/30\n","95/95 [==============================] - 44s 466ms/step - loss: 0.6945 - accuracy: 0.5314 - val_loss: 0.6948 - val_accuracy: 0.5337\n","Epoch 12/30\n","95/95 [==============================] - 44s 467ms/step - loss: 0.6931 - accuracy: 0.5261 - val_loss: 0.6909 - val_accuracy: 0.5337\n","Epoch 13/30\n","95/95 [==============================] - 44s 459ms/step - loss: 0.6928 - accuracy: 0.5235 - val_loss: 0.6909 - val_accuracy: 0.5337\n","Epoch 14/30\n","95/95 [==============================] - 44s 466ms/step - loss: 0.6917 - accuracy: 0.5314 - val_loss: 0.6927 - val_accuracy: 0.5337\n","Epoch 15/30\n","95/95 [==============================] - 44s 466ms/step - loss: 0.6928 - accuracy: 0.5314 - val_loss: 0.6915 - val_accuracy: 0.5337\n","Epoch 16/30\n","95/95 [==============================] - 44s 466ms/step - loss: 0.6917 - accuracy: 0.5314 - val_loss: 0.6942 - val_accuracy: 0.5337\n","Epoch 17/30\n","95/95 [==============================] - 44s 467ms/step - loss: 0.6921 - accuracy: 0.5235 - val_loss: 0.6937 - val_accuracy: 0.5337\n","Epoch 18/30\n","95/95 [==============================] - 44s 463ms/step - loss: 0.6921 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5337\n","Epoch 19/30\n","95/95 [==============================] - 44s 467ms/step - loss: 0.6921 - accuracy: 0.5314 - val_loss: 0.6909 - val_accuracy: 0.5337\n","Epoch 20/30\n","95/95 [==============================] - 44s 460ms/step - loss: 0.6916 - accuracy: 0.5314 - val_loss: 0.6909 - val_accuracy: 0.5337\n","Epoch 21/30\n","95/95 [==============================] - 44s 466ms/step - loss: 0.6916 - accuracy: 0.5314 - val_loss: 0.6909 - val_accuracy: 0.5337\n","Epoch 22/30\n","95/95 [==============================] - 44s 460ms/step - loss: 0.7081 - accuracy: 0.5056 - val_loss: 0.7364 - val_accuracy: 0.4663\n","Epoch 23/30\n","95/95 [==============================] - 44s 466ms/step - loss: 0.7067 - accuracy: 0.5288 - val_loss: 0.6915 - val_accuracy: 0.5337\n"]}],"source":["bias_model.fit(train_bias, epochs=30, validation_data = val_bias)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiNYWNBqfpGb"},"outputs":[],"source":["bias_model.save('bias_model.keras')\n","files.download('bias_model.keras')"]},{"cell_type":"markdown","metadata":{"id":"--oaL4MssMbZ"},"source":["## Neutrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12A3-nXh0lL7"},"outputs":[],"source":["def create_neutrality_model(max_len):\n","  input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n","  attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n","\n","  bert = TFBertModel.from_pretrained(bert_base, from_pt=True)(input_ids, attention_mask)\n","  classifier = keras.Sequential([\n","        layers.Dense(32, input_shape=(768,), activation=\"relu\"),\n","        layers.Dense(32, activation=\"relu\"),\n","        layers.Dense(3, activation='softmax')\n","  ])(bert.pooler_output)\n","\n","  bert.trainable = False\n","\n","  model = keras.Model(inputs=[input_ids, attention_mask],\n","                     outputs=classifier)\n","\n","  model.compile(loss='categorical_crossentropy',\n","                optimizer='adam',\n","                metrics=['accuracy'])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3362,"status":"ok","timestamp":1701828974200,"user":{"displayName":"Virdian Harun Prayoga M010BSY0110","userId":"05344229873782265456"},"user_tz":-420},"id":"FR_mRxQE0lL8","outputId":"0c4585d5-9d77-44e5-9623-e0da0f22312c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_ids (InputLayer)      [(None, 128)]                0         []                            \n","                                                                                                  \n"," attention_mask (InputLayer  [(None, 128)]                0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," tf_bert_model_1 (TFBertMod  TFBaseModelOutputWithPooli   1105582   ['input_ids[0][0]',           \n"," el)                         ngAndCrossAttentions(last_   08         'attention_mask[0][0]']      \n","                             hidden_state=(None, 128, 7                                           \n","                             68),                                                                 \n","                              pooler_output=(None, 768)                                           \n","                             , past_key_values=None, hi                                           \n","                             dden_states=None, attentio                                           \n","                             ns=None, cross_attentions=                                           \n","                             None)                                                                \n","                                                                                                  \n"," sequential_1 (Sequential)   (None, 3)                    25763     ['tf_bert_model_1[0][1]']     \n","                                                                                                  \n","==================================================================================================\n","Total params: 110583971 (421.84 MB)\n","Trainable params: 110583971 (421.84 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["neutrality_model = create_neutrality_model(max_len_neutrality)\n","neutrality_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2h6WbPlj8Q0D"},"outputs":[],"source":["neutrality_model.fit(train_neutrality, epochs=30, validation_data = val_neutrality)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["wNdtUxlV36co","TMmooS0xRZ7e","EC4JAkZrzdXp","F41sSm-q5vH4","t-ZkWW4E56vS","tCqdJlHIVH5X"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1c241ae5a70c4f72aadd4aa237af80ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29978b5b20984dd5be8a773787857116","IPY_MODEL_0a49668e7ac1405782b5e4db65d63fd1","IPY_MODEL_a638c4b74d6b49f78d4463964312afd6"],"layout":"IPY_MODEL_682ed2253eba4e4f9585a1b606de29ad"}},"29978b5b20984dd5be8a773787857116":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7529e77a5004c1c83bf61077b1819fe","placeholder":"​","style":"IPY_MODEL_90691a9cd67d4f50bf51d33e8efe29e3","value":"pytorch_model.bin: 100%"}},"0a49668e7ac1405782b5e4db65d63fd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68707cfa67824fa48a83e2b0f6929d42","max":444780374,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2df9c08c75c845e680297cd0e7f97703","value":444780374}},"a638c4b74d6b49f78d4463964312afd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cef09447899549f685b3791158e182da","placeholder":"​","style":"IPY_MODEL_9d53591ada9a4bccba121ab0f34000f1","value":" 445M/445M [00:03&lt;00:00, 161MB/s]"}},"682ed2253eba4e4f9585a1b606de29ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7529e77a5004c1c83bf61077b1819fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90691a9cd67d4f50bf51d33e8efe29e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68707cfa67824fa48a83e2b0f6929d42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2df9c08c75c845e680297cd0e7f97703":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cef09447899549f685b3791158e182da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d53591ada9a4bccba121ab0f34000f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}